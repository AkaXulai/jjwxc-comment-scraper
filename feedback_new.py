import time
import bs4
import os
import requests
import re
import threading
import logging
import html2text
import pandas as pd
import random
from concurrent.futures import ThreadPoolExecutor
import streamlit as st
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)

# è®¾ç½® Streamlit ç•Œé¢
st.title("æ™‹æ±Ÿè¯„è®ºçˆ¬è™«å°åŠ©æ‰‹")

# è¾“å…¥ä½œå“ ID å’Œç« èŠ‚èŒƒå›´
novel_id = st.text_input("è¯·è¾“å…¥ä½œå“IDï¼š", "")
chapter_range_input = st.text_input("è¯·è¾“å…¥ç« èŠ‚èŒƒå›´ï¼ˆä¾‹å¦‚ï¼š1-5 æˆ– 1,3,5ï¼‰ï¼š", "")

# æå–ç« èŠ‚èŒƒå›´
def parse_chapter_range(chapter_range_input):
    chapter_range = []
    try:
        if '-' in chapter_range_input:  # å¤„ç†è¿ç»­åŒºé—´
            start, end = map(int, chapter_range_input.split('-'))
            chapter_range = list(range(start, end + 1))
        elif ',' in chapter_range_input:  # å¤„ç†åˆ†å¼€çš„ç« èŠ‚å·
            chapter_range = list(map(int, chapter_range_input.split(',')))
        else:  # å¦‚æœæ˜¯å•ä¸ªç« èŠ‚
            chapter_range = [int(chapter_range_input)]
    except ValueError:
        st.error("ç« èŠ‚èŒƒå›´æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„èŒƒå›´ï¼ˆä¾‹å¦‚ï¼š1-5 æˆ– 1,3,5ï¼‰ã€‚")
    return chapter_range

# æ ¸å¿ƒçˆ¬å–åŠŸèƒ½
def create_session():
    session = requests.Session()
    retry = Retry(
        total=3,  # é‡è¯•æ¬¡æ•°
        backoff_factor=1,  # é‡è¯•å»¶è¿Ÿæ—¶é—´
        status_forcelist=[500, 502, 503, 504],  # é‡è¯•çš„ HTTP çŠ¶æ€ç 
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    return session

# è·å–ç« èŠ‚æ ‡é¢˜
def get_chapter_titles_v2(novel_id):
    global chapter_titles
    chapter_titles = {}
    try:
        logging.info(f"å¼€å§‹çˆ¬å–å°è¯´ {novel_id} çš„ç« èŠ‚æ ‡é¢˜...")
        session = create_session()
        response = session.get(
            f"https://www.jjwxc.net/onebook.php?novelid={novel_id}",
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
            },
            timeout=15,
        )

        # æ”¹ä¸ºä½¿ç”¨ "html.parser" è§£æå™¨
        soup = bs4.BeautifulSoup(response.content.decode("gbk", errors="ignore"), "html.parser")

        rows = soup.select("tr")
        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 2:
                continue

            chapter_id = cells[0].get_text(strip=True)
            chapter_title = cells[1].get_text(strip=True)

            if chapter_id.isdigit():
                chapter_titles[int(chapter_id)] = chapter_title

        logging.info(f"æˆåŠŸæå–ç« èŠ‚æ ‡é¢˜ï¼Œå…± {len(chapter_titles)} ç« ã€‚")
    except Exception as e:
        logging.error(f"æå–ç« èŠ‚æ ‡é¢˜å¤±è´¥: {e}")

# è·å–è¯„è®º
def get_comments_for_chapter(chapter_id, cookies=""):
    comments_data = []
    try:
        page = 1
        while True:
            logging.info(f"æ­£åœ¨è·å–ç¬¬ {chapter_id} ç« ï¼Œç¬¬ {page} é¡µè¯„è®º...")
            response = requests.get(
                "https://www.jjwxc.net/comment.php",
                params={
                    "novelid": novel_id,
                    "chapterid": chapter_id,
                    "page": page,
                },
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
                    "Cookie": cookies,
                },
                timeout=15,
            )

            # æ”¹ä¸ºä½¿ç”¨ "html.parser" è§£æå™¨
            soup = bs4.BeautifulSoup(response.content.decode("gbk", errors="ignore"), "html.parser")
            comment_divs = soup.find_all("div", id=re.compile(r"comment_\d+"))

            if not comment_divs:
                logging.info(f"ç¬¬ {chapter_id} ç« çš„ç¬¬ {page} é¡µæ— è¯„è®ºï¼Œç»“æŸæœ¬ç« çˆ¬å–ã€‚")
                break

            for comment in comment_divs:
                try:
                    comment_text = html2text.html2text(str(comment))
                    time_re = re.compile(r"å‘è¡¨æ—¶é—´ï¼š[0-9\-\s:]*")
                    name_re = re.compile(r"ç½‘å‹ï¼š\[[\s\S]*?\]")

                    comment_time = time_re.findall(comment_text)[0][5:].strip()
                    try:
                        commenter_name = name_re.findall(comment_text)[0][3:].strip()
                    except IndexError:
                        commenter_name = "åŒ¿åç”¨æˆ·"

                    chapter_title = chapter_titles.get(chapter_id, "æœªçŸ¥ç« èŠ‚")

                    chapter_label = f"ç¬¬{chapter_id}ç«  {chapter_title}"

                    comments_data.append([comment_time, commenter_name, comment_text, chapter_label, page])

                except Exception as e:
                    logging.error(f"è§£æè¯„è®ºå¤±è´¥: {e}")

            page += 1
            time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿ

        return comments_data
    except Exception as e:
        logging.error(f"çˆ¬å–ç« èŠ‚ {chapter_id} è¯„è®ºå¤±è´¥: {e}")
        return []

# æ‰§è¡Œçˆ¬å–
def run_crawler(novel_id, chapter_range):
    get_chapter_titles_v2(novel_id)
    
    all_comments = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(get_comments_for_chapter, chapter_range)
        for result in results:
            all_comments.extend(result)
            time.sleep(random.uniform(1, 3))  # é˜²æ­¢è¿‡äºé¢‘ç¹çš„è¯·æ±‚

    return all_comments

# æ•°æ®å¤„ç†ä¸å¯¼å‡º
def export_to_excel(comments_data):
    df = pd.DataFrame(comments_data, columns=["è¯„è®ºæ—¶é—´", "è¯„è®ºè€…", "è¯„è®ºå†…å®¹", "ç« èŠ‚", "é¡µç "])
    output_file = f'novel_{novel_id}_comments_{time.strftime("%Y%m%d_%H%M%S")}.xlsx'
    df.to_excel(output_file, index=False, engine='openpyxl')

    return output_file

# Streamlit é¡µé¢äº¤äº’
if st.button("å¼€å§‹çˆ¬å–"):
    if not novel_id or not chapter_range_input:
        st.error("è¯·è¾“å…¥ä½œå“IDå’Œç« èŠ‚èŒƒå›´")
    else:
        chapter_range = parse_chapter_range(chapter_range_input)
        if chapter_range:
            with st.spinner("æ­£åœ¨çˆ¬å–æ•°æ®..."):
                all_comments = run_crawler(novel_id, chapter_range)

            if all_comments:
                output_file = export_to_excel(all_comments)
                st.success(f"è¯„è®ºæ•°æ®å·²æˆåŠŸä¿å­˜ï¼æ–‡ä»¶ï¼š{output_file}")
                st.download_button(label="ä¸‹è½½è¯„è®ºæ•°æ®", data=open(output_file, "rb"), file_name=output_file)
        else:
            st.error("ç« èŠ‚èŒƒå›´æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„èŒƒå›´ï¼ˆä¾‹å¦‚ï¼š1-5 æˆ– 1,3,5ï¼‰ã€‚")

# ç¡®ä¿æ–‡ä»¶å­˜åœ¨
def ensure_files_exist():
    if not os.path.exists("messages.txt"):
        with open("messages.txt", "w", encoding="utf-8"):
            pass
    if not os.path.exists("replies.txt"):
        with open("replies.txt", "w", encoding="utf-8"):
            pass

ensure_files_exist()

# åˆ›å»ºä¸€ä¸ªé”æ¥ä¿è¯çº¿ç¨‹å®‰å…¨ 
message_lock = threading.Lock()

# ä¿å­˜ç•™è¨€
def save_message(name, message):
    with message_lock:
        with open("messages.txt", "a", encoding="utf-8") as file:
            file.write(f"{name}: {message}\n")
    st.write(f"å·²ä¿å­˜ç•™è¨€ï¼š{name}: {message}")  # è°ƒè¯•ä¿¡æ¯

# ä¿å­˜å›å¤
def save_reply_to_file(reply_index, reply_text):
    with message_lock:
        with open("replies.txt", "a", encoding="utf-8") as file:
            file.write(f"è¯„è®º {reply_index}: {reply_text}\n")

# Streamlit é¡µé¢äº¤äº’éƒ¨åˆ†
st.title("ğŸ’¬ ç•™è¨€äº’åŠ¨")

# ç”¨æˆ·è¾“å…¥ç•™è¨€
name = st.text_input("ä½ çš„æ˜µç§°ï¼š", "")
message = st.text_area("æƒ³å¯¹æˆ‘ä»¬è¯´ç‚¹ä»€ä¹ˆï¼š")

if st.button("æäº¤ç•™è¨€"):
    if message.strip():
        save_message(name, message)
        st.success("è°¢è°¢ä½ çš„ç•™è¨€ï¼æˆ‘ä»¬ä¼šè®¤çœŸé˜…è¯»çš„ ğŸ˜Š")
    else:
        st.error("ç•™è¨€ä¸èƒ½ä¸ºç©ºå“¦ï¼")

# æ˜¾ç¤ºç•™è¨€å’Œæ¥¼ä¸­æ¥¼å›å¤
st.write("### ğŸ“ ç•™è¨€æ¿")

try:
    with open("messages.txt", "r", encoding="utf-8") as msg_file:
        messages = msg_file.readlines()

    with open("replies.txt", "r", encoding="utf-8") as reply_file:
        replies = reply_file.readlines()

except FileNotFoundError:
    messages = []
    replies = []

# åˆ†é¡µåŠŸèƒ½
PAGE_SIZE = 5
if len(messages) > 0:  
    max_page = (len(messages) // PAGE_SIZE) + 1 if len(messages) % PAGE_SIZE != 0 else len(messages) // PAGE_SIZE
    max_page = max(max_page, 1)

    # å¦‚æœåªæœ‰ä¸€é¡µï¼Œç›´æ¥æ˜¾ç¤ºç¬¬ä¸€é¡µï¼Œä¸ç”¨slider
    if max_page > 1:
        page_num = st.slider("é€‰æ‹©ä½ æƒ³çœ‹çš„ç•™è¨€é¡µ", 1, max_page, 1)
    else:
        page_num = 1  # å¦‚æœåªæœ‰ä¸€é¡µï¼Œç›´æ¥è®¾ç½®ä¸º1
else:
    page_num = 1  # å¦‚æœæ²¡æœ‰ç•™è¨€ï¼Œé»˜è®¤æ˜¾ç¤ºç¬¬ä¸€é¡µ
    max_page = 1  # è®¾ç½®æœ€å¤§é¡µæ•°ä¸º1
    st.write("ç›®å‰æ²¡æœ‰ç•™è¨€å“¦")

# å½“å‰é¡µçš„ç•™è¨€
start_idx = (page_num - 1) * PAGE_SIZE
end_idx = start_idx + PAGE_SIZE
paged_messages = messages[start_idx:end_idx]

st.write(f"ğŸ“„ **æ˜¾ç¤ºç•™è¨€ï¼šç¬¬ {page_num} é¡µï¼Œå…± {max_page} é¡µ**")

# æ˜¾ç¤ºç•™è¨€å’Œå›å¤
for idx, msg in enumerate(paged_messages):
    user_name, msg_text = msg.split(":", 1)
    related_replies = [r for r in replies if f"è¯„è®º {idx + 1}:" in r]
    reply_text = related_replies[0].strip() if related_replies else "æš‚æœªå›å¤"

    st.write(f"**{user_name} çš„ç•™è¨€ï¼š** {msg_text.strip()}")
    st.write(f"**^ ^ å›å¤ï¼š** {reply_text}")

    # ç•™è¨€å›å¤éƒ¨åˆ†
    reply_index = idx + 1
    reply_text = st.text_area(f"å¯¹ {user_name} çš„ç•™è¨€å›å¤ï¼š", key=f"reply_{reply_index}")
    if st.button(f"æäº¤å›å¤ç»™ {user_name}", key=f"submit_reply_{reply_index}"):

        if reply_text.strip():
            save_reply_to_file(reply_index, reply_text)
            st.success("å›å¤æˆåŠŸï¼ ğŸ˜Š")
        else:
            st.error("å›å¤ä¸èƒ½ä¸ºç©ºå“¦ï¼")

# åœ¨ Streamlit é¡µé¢æ˜¾ç¤ºç»“å°¾ä¿¡æ¯
st.write("---")
st.header("ğŸ‰ å®Œæˆå•¦ï¼")
st.markdown("""
    ä¹…ç­‰å•¦ï¼Œå˜¿å˜¿~ ä½¿ç”¨æŒ‡å—å’Œä¸Šæ¬¡çš„å›å¤å·²ç»ä¸€èµ·æ”¾åˆ°è¿™ä¸ªæ–‡æ¡£å•¦~
    è¯·æŸ¥çœ‹ï¼š[ä½¿ç”¨æŒ‡å—ä¸å›å¤](https://docs.qq.com/doc/DT0pkdWR1UkVUZGJx)
""")
