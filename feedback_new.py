import time
import bs4
import requests
import re
import logging
import html2text
import pandas as pd
import random
from concurrent.futures import ThreadPoolExecutor
import streamlit as st
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)

# Streamlit é¡µé¢è®¾ç½®
st.title("æ™‹æ±Ÿè¯„è®ºå°åŠ©æ‰‹")
st.write("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼Œè¿™é‡Œå¯ä»¥å¸®ä½ å¿«é€Ÿçˆ¬å–æ™‹æ±Ÿæ–‡å­¦åŸçš„ç« èŠ‚è¯„è®ºå¹¶å¯¼å‡ºæˆ Excel æ–‡ä»¶ï¼")

# è¾“å…¥å°è¯´ ID å’Œç« èŠ‚èŒƒå›´
novel_id = st.text_input("ğŸ“˜ è¯·è¾“å…¥ä½œå“ IDï¼š", "")
chapter_range_input = st.text_input("ğŸ“– è¯·è¾“å…¥ç« èŠ‚èŒƒå›´ï¼ˆä¾‹å¦‚ï¼š1-5 æˆ– 1,3,5ï¼‰ï¼š", "")

# æå–ç« èŠ‚èŒƒå›´
def parse_chapter_range(chapter_range_input):
    chapter_range = []
    try:
        if '-' in chapter_range_input:  # è¿ç»­åŒºé—´
            start, end = map(int, chapter_range_input.split('-'))
            chapter_range = list(range(start, end + 1))
        elif ',' in chapter_range_input:  # æŒ‡å®šç« èŠ‚
            chapter_range = list(map(int, chapter_range_input.split(',')))
        else:  # å•ä¸€ç« èŠ‚
            chapter_range = [int(chapter_range_input)]
    except ValueError:
        st.error("ç« èŠ‚èŒƒå›´æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„èŒƒå›´ï¼ˆä¾‹å¦‚ï¼š1-5 æˆ– 1,3,5ï¼‰ã€‚")
    return chapter_range

# åˆ›å»ºä¼šè¯
def create_session():
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    return session

# è·å–ç« èŠ‚æ ‡é¢˜
def get_chapter_titles_v2(novel_id):
    global chapter_titles
    chapter_titles = {}
    try:
        logging.info(f"æ­£åœ¨è·å–å°è¯´ {novel_id} çš„ç« èŠ‚æ ‡é¢˜...")
        session = create_session()
        response = session.get(
            f"https://www.jjwxc.net/onebook.php?novelid={novel_id}",
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=15,
        )

        soup = bs4.BeautifulSoup(response.content.decode("gbk", errors="ignore"), "html.parser")
        rows = soup.select("tr")
        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 2:
                continue

            chapter_id = cells[0].get_text(strip=True)
            chapter_title = cells[1].get_text(strip=True)

            if chapter_id.isdigit():
                chapter_titles[int(chapter_id)] = chapter_title

        logging.info(f"è·å–åˆ° {len(chapter_titles)} ä¸ªç« èŠ‚æ ‡é¢˜ã€‚")
    except Exception as e:
        logging.error(f"è·å–ç« èŠ‚æ ‡é¢˜å¤±è´¥: {e}")

# è·å–è¯„è®º
def get_comments_for_chapter(chapter_id, cookies=""):
    comments_data = []
    try:
        page = 1
        while True:
            logging.info(f"æ­£åœ¨è·å–ç¬¬ {chapter_id} ç« ï¼Œç¬¬ {page} é¡µçš„è¯„è®º...")
            response = requests.get(
                "https://www.jjwxc.net/comment.php",
                params={"novelid": novel_id, "chapterid": chapter_id, "page": page},
                headers={"User-Agent": "Mozilla/5.0", "Cookie": cookies},
                timeout=15,
            )

            soup = bs4.BeautifulSoup(response.content.decode("gbk", errors="ignore"), "html.parser")
            comment_divs = soup.find_all("div", id=re.compile(r"comment_\\d+"))

            if not comment_divs:
                logging.info(f"ç¬¬ {chapter_id} ç« ï¼Œç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šè¯„è®ºã€‚")
                break

            for comment in comment_divs:
                try:
                    comment_text = html2text.html2text(str(comment))
                    time_re = re.compile(r"å‘è¡¨æ—¶é—´ï¼š[0-9\-\s:]*")
                    name_re = re.compile(r"ç½‘å‹ï¼š\[[\s\S]*?\]")

                    comment_time = time_re.findall(comment_text)[0][5:].strip()
                    commenter_name = name_re.findall(comment_text)[0][3:].strip() if name_re.findall(comment_text) else "åŒ¿åç”¨æˆ·"

                    chapter_title = chapter_titles.get(chapter_id, "æœªçŸ¥ç« èŠ‚")
                    chapter_label = f"ç¬¬{chapter_id}ç«  {chapter_title}"

                    comments_data.append([comment_time, commenter_name, comment_text, chapter_label, page])
                except Exception as e:
                    logging.error(f"è§£æè¯„è®ºå¤±è´¥: {e}")

            page += 1
            time.sleep(random.uniform(1, 3))

        return comments_data
    except Exception as e:
        logging.error(f"çˆ¬å–ç¬¬ {chapter_id} ç« è¯„è®ºå¤±è´¥: {e}")
        return []

# æ‰§è¡Œçˆ¬å–
def run_crawler(novel_id, chapter_range):
    get_chapter_titles_v2(novel_id)
    all_comments = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(get_comments_for_chapter, chapter_range)
        for result in results:
            all_comments.extend(result)
            time.sleep(random.uniform(1, 3))

    return all_comments

# å¯¼å‡ºè¯„è®ºæ•°æ®åˆ° Excel
def export_to_excel(comments_data):
    df = pd.DataFrame(comments_data, columns=["è¯„è®ºæ—¶é—´", "è¯„è®ºè€…", "è¯„è®ºå†…å®¹", "ç« èŠ‚", "é¡µç "])
    output_file = f'novel_{novel_id}_comments_{time.strftime("%Y%m%d_%H%M%S")}.xlsx'
    df.to_excel(output_file, index=False, engine='openpyxl')
    return output_file

# Streamlit ç•Œé¢äº¤äº’
if st.button("å¼€å§‹çˆ¬å–"):
    if not novel_id or not chapter_range_input:
        st.error("â— è¯·å¡«å†™ä½œå“ ID å’Œç« èŠ‚èŒƒå›´ã€‚")
    else:
        chapter_range = parse_chapter_range(chapter_range_input)
        if chapter_range:
            with st.spinner("â³ æ­£åœ¨åŠªåŠ›çˆ¬å–æ•°æ®ä¸­ï¼Œè¯·ç¨å€™..."):
                all_comments = run_crawler(novel_id, chapter_range)

            if all_comments:
                output_file = export_to_excel(all_comments)
                st.success("âœ… è¯„è®ºçˆ¬å–å®Œæˆï¼")
                st.download_button(
                    label="ğŸ“‚ ç‚¹å‡»ä¸‹è½½è¯„è®ºæ•°æ®", 
                    data=open(output_file, "rb"), 
                    file_name=output_file
                )
            else:
                st.warning("âš ï¸ æœªèƒ½è·å–åˆ°ä»»ä½•è¯„è®ºæ•°æ®ï¼Œè¯·æ£€æŸ¥è¾“å…¥ä¿¡æ¯ã€‚")
        else:
            st.error("âŒ ç« èŠ‚èŒƒå›´æ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

# æ·»åŠ ç•™è¨€äº’åŠ¨åŒº
st.write("---")
st.header("ğŸ’¬ ç•™è¨€äº’åŠ¨")

# ä¿å­˜ç•™è¨€åŠå›å¤
def save_message(name, message):
    with open("messages.txt", "a", encoding="utf-8") as file:
        file.write(f"{name}: {message}\n")

def save_reply(index, reply):
    with open("replies.txt", "a", encoding="utf-8") as file:
        file.write(f"{index}: {reply}\n")

# ç”¨æˆ·è¾“å…¥ç•™è¨€
name = st.text_input("ä½ çš„åå­—ï¼š", "åŒ¿åç”¨æˆ·")
message = st.text_area("æƒ³å¯¹æˆ‘ä»¬è¯´ç‚¹ä»€ä¹ˆï¼š")

if st.button("æäº¤ç•™è¨€"):
    if message.strip():
        save_message(name, message)
        st.success("è°¢è°¢ä½ çš„ç•™è¨€ï¼æˆ‘ä»¬ä¼šè®¤çœŸé˜…è¯»çš„ ğŸ˜Š")
    else:
        st.error("ç•™è¨€ä¸èƒ½ä¸ºç©ºå“¦ï¼")

# æ˜¾ç¤ºç•™è¨€åŠç®¡ç†å‘˜å›å¤
st.write("### ğŸ“ ç•™è¨€æ¿")
try:
    with open("messages.txt", "r", encoding="utf-8") as msg_file:
        messages = msg_file.readlines()

    with open("replies.txt", "r", encoding="utf-8") as reply_file:
        replies = reply_file.readlines()
except FileNotFoundError:
    messages = []
    replies = []

if messages:
    for idx, msg in enumerate(messages):
        st.write(f"{idx + 1}. {msg.strip()}")
        corresponding_reply = next((r.split(": ", 1)[1].strip() for r in replies if r.startswith(f
